

<html>
    <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
      <!--
      <script src="./resources/jsapi" type="text/javascript"></script>
      <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
     -->
    
    <style type="text/css">
      body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
        text-align: justify;
      }
      h1 {
        font-weight:300;
      }
      h2 {
        font-weight:300;
      }
      .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
      }
      video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      a:link,a:visited
      {
        color: #1367a7;
        text-decoration: none;
      }
      a:hover {
        color: #208799;
      }
      td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
      }
      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
      }
      .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
      }
      .vert-cent {
        position: relative;
          top: 50%;
          transform: translateY(-50%);
      }
      hr
      {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
      }
    </style>
    
    
    
        <title>Rewriting Geometric Rules of a GAN</title>
        <meta property="og:image" content="http://peterwang512.github.io/GANWarping/files/thumbnail.png">
        <meta property="og:title" content="Rewriting Geometric Rules of a GAN">
      </head>
    
      <body>
            <br>
              <center>
                <span style="font-size:34px">Rewriting Geometric Rules of a GAN</span><br><br>
    
              <table align="center" width="850px">
                <tbody><tr>
                        <td align="center" width="205px">
                  <center>
                    <span style="font-size:20px"><a href="http://peterwang512.github.io">Sheng-Yu Wang</a><sup>1</sup></span>
                    </center>
                    </td>
                        <td align="center" width="175px">
                  <center>
                    <span style="font-size:20px"><a href="https://baulab.info/">David Bau</a><sup>2</sup></span>
                    </center>
                    </td>
                        <td align="center" width="175px">
                  <center>
                    <span style="font-size:20px"><a href="http://cs.cmu.edu/~junyanz">Jun-Yan Zhu</a><sup>1</sup></span>
                    </center>
                    </td>
                </tr>
            </tbody></table>
    
              <table align="center" width="700px">
                <tbody><tr>
                        <td align="center" width="100px">
                  <center>
                        <span style="font-size:20px"></span>
                    </center>
                    </td>
                        <td align="center" width="250px">
                  <center>
                        <span style="font-size:20px"><sup>1</sup>CMU</span>
                    </center>
                    </td>
                        <td align="center" width="250px">
                  <center>
                        <span style="font-size:20px"><sup>2</sup>Northeastern University</span>
                    </center>
                    </td>
                        <td align="center" width="100px">
                  <center>
                        <span style="font-size:20px"></span>
                    </center>
                    </td>
            </tr></tbody></table>
    
              <table align="center" width="700px">
                <tbody><tr>
    <!--                     <td align="center" width="50px">
                  <center>
                        <span style="font-size:18px"></span>
                    </center>
                    </td> -->
                        <td align="center" width="200px">
                  <center>
                    <br>
                    <span style="font-size:20px">Code <a href="https://github.com/peterwang512/GANWarping"> [GitHub]</a></span>
                    </center>
                    </td>
                        <td align="center" width="200px">
                  <center>
                    <br>
                    <span style="font-size:20px">SIGGRAPH 2022 <a href="https://arxiv.org/abs/2207.14288"> <!-- [Paper] -->[Paper]</a></span>
                    </center>
                    </td>
                        <td align="center" width="200px">
                  <center>
                    <br>
                    <span style="font-size:20px">Slides <a href="https://drive.google.com/file/d/10BD5-7WewE5ASyVqUUl65LopF8e5wJgZ/view"> [pptx]</a></span>
                    </center>
                    </td>
    
            </tr></tbody></table>
              </center>
            <br>
            <table align="center" width="1000px">
              <tbody><tr>
                      <td width="400px">
                <center>
                    <video id="teaser_video" width="720px" loop src="./files/teaser_video.mp4" autoplay muted>
                        Your browser does not support HTML5 Player 
                    </video>
                          <!-- <img class="rounded" src="./files/teaser_video.mp4" width="1000px"> -->
                </center>
                      </td>
                      </tr>
                      </tbody></table>
                <br>
                With our method, a user can edit a GAN model to synthesize many unseen objects with the desired shape. The user is asked to warp just a handful of generated images by defining several control points to obtain the customized models. While the edited models change an objectâ€™s shape, other visual cues, such as pose, color, texture, and background, are faithfully preserved after the modification.
                <br>
              <br>
          <hr>
    
            <center><h2>Abstract</h2></center>
            Deep generative models make visual content creation more accessible to novice users by automating the synthesis of diverse, realistic content based on a collected dataset. However, the current machine learning approaches miss a key element of the creative process -- the ability to synthesize things that go far beyond the data distribution and everyday experience. To begin to address this issue, we enable a user to "warp" a given model by editing just a handful of original model outputs with desired geometric changes. Our method applies a low-rank update to a single model layer to reconstruct edited examples. Furthermore, to combat overfitting, we propose a latent space augmentation method based on style-mixing. Our method allows a user to create a model that synthesizes endless objects with defined geometric changes, enabling the creation of a new generative model without the burden of curating a large-scale dataset. We also demonstrate that edited models can be composed to achieve aggregated effects, and we present an interactive interface to enable users to create new models through composition. Empirical measurements on multiple test cases suggest the advantage of our method against recent GAN fine-tuning methods. Finally, we showcase several applications using the edited models, including latent space interpolation and image editing. <br>

            <hr><center><h2>Comparison with text-to-image model</h2></center>
            Our method enables editing modalities that are difficult to be described by text. It is unnatural to describe the warping edits precisely using text. To showcase this, we compare our edited models with DALLE-2. Here we attempted to provide the text prompts that best match the warping edits. Despite this, we observe that DALLE-2 leads to unintended color and texture changes, while our method yields consistent shape changes throughout all model samples.<br><br>
            <center><img src="files/dalle-rabbit.png" width="800px"/></center> <hr>
            <center><img src="files/dalle-alien.png" width="800px"/></center>
            <br>
    
            <hr><center><h2 id="slides_video">Video</h2></center>
            <p align="center">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/2m7_rbsO6Hk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </p>
              <center>
                <span style="font-size:20px"><a href="https://drive.google.com/file/d/10BD5-7WewE5ASyVqUUl65LopF8e5wJgZ/view">[Slides]</a></span>
              </center>
    
    
        <br>
    
          <hr>
            <center><h2>Paper</h2></center><table align="center" width="700" px="">
    
              <tbody><tr>
              <td><a href="https://arxiv.org/abs/2207.14288"><img class="layered-paper-big" style="height:175px" src="./files/firstpg.png"></a></td>
              <td><span style="font-size:12pt">Sheng-Yu Wang, David Bau, Jun-Yan Zhu.</span><br>
              <b><span style="font-size:12pt">Rewriting Geometric Rules of a GAN.</span></b><br>
              <span style="font-size:12pt">In SIGGRAPH, 2022. (<a href="https://arxiv.org/abs/2207.14288">Paper</a>)</span>
              </td>
    
              <br>
              <table align="center" width="600px">
                <tbody>
                  <tr>
                    <td>
                      <center>
                        <span style="font-size:22px">
                          <a href="./files/bibtex.txt" target="_blank">[Bibtex]</a>
                        </span>
                      </center>
                    </td>
                  </tr>
                </tbody>
              </table>
          <br>
    
    
          <br><hr>

        <center><h2>Method</h2></center><table align="center" width="700" px="">
            <p>A user first edits a handful of samples from the pre-trained generative model. We then train a customized model so that it can synthesize new samples with a similar visual effect specified by the user edit. To prevent overfitting, we apply style-mixing augmentation to the edited samples. For each sample, we mix the original latent code with a new randomly sampled texture latent code. Since the augmented samples still preserve shapes and poses, we can apply the same user edit to obtain a training set with diverse texture variations. We learn the customized model on the augmented training set using the LPIPS.</p>
          <center>
            <video width="720" muted controls preload="metadata">
                <source src="files/method.mp4"
                        type="video/mp4">
                Your browser does not support the video tag.
            </video>
                  <!-- <img class="rounded" src="./files/teaser_video.mp4" width="1000px"> -->
            </center>
        <br><hr>
    
          <center><h2>Results</h2></center>
    
    
           <p align="center"><b>Warp edits.</b> Below we show warped models with different object categories.</p>
           
           <center>
            <video width="720" muted autoplay controls preload="metadata">
                <source src="files/warp.mp4"
                        type="video/mp4">
                Your browser does not support the video tag.
            </video>
                  <!-- <img class="rounded" src="./files/teaser_video.mp4" width="1000px"> -->
            </center>
    
    
           <p align="center"><b>Color edits.</b> Our method can also be applied to color edits. The colored strokes specify the locations to perform coloring changes, while the darker region defines the region to be preserved. The edited models produce precise coloring changes in the specified parts.</p>
           <center>
            <video width="720" muted autoplay controls preload="metadata">
                <source src="files/color.mp4"
                        type="video/mp4">
                Your browser does not support the video tag.
            </video>
                  <!-- <img class="rounded" src="./files/teaser_video.mp4" width="1000px"> -->
            </center>
    

        <p align="center"><b>Compose edited models.</b> We can compose the edited models into a new model with aggregated geometric changes, by simpling blending the model weights linearly. We present an interface for users to easily create a new model by composing the edited models made beforehand. Please visit <a href="https://github.com/PeterWang512/GANWarping/blob/main/notebooks/compose_models.ipynb">this notebook</a> for more details. </p>
        <center>
            <video width="720" muted autoplay controls preload="metadata">
                <source src="files/interface.mp4"
                        type="video/mp4">
                Your browser does not support the video tag.
            </video>
                  <!-- <img class="rounded" src="./files/teaser_video.mp4" width="1000px"> -->
            </center>

        <p align="center"><b>Latent space edits.</b>  Our edited models can generate smooth transitions between two random samples by interpolating the latent space. We can also apply <a href="https://github.com/harskish/ganspace">GANSpace</a> edits to our models to change the object attributes such as poses or colors.</p>
    
        <center><img src="files/edit.jpg" width="800px"/></center>
    
    <br><hr>
    <table align="center" width="1000px">
      <tbody><tr>
              <td width="400px">
        <left>
      <center><h2>Related works</h2></center>
    </left>
    <ul>
      <li>Y. Nitzan, K. Aberman, Q. He, O. Liba, M. Yarom, Y. Gandelsman, I. Mosseri, Y. Pritch, D. Cohen-or. <a href="https://arxiv.org/abs/2203.17272">"MyStyle: A Personalized Generative Prior".</a>. In ArXiv.</li>
      <li>R. Gal, O. Patashnik, H. Maron, A. Bermano, G. Chechik, D. Cohen-Or. <a href="https://arxiv.org/abs/2108.00946">"StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators."</a>. In SIGGRAPH 2022.</li><br>
      <li>S.-Y. Wang, D. Bau, J.-Y. Zhu. <a href="https://arxiv.org/abs/2108.02774">"Sketch Your Own GAN"</a>. In ICCV 2021.</li><br>
      <li>D. Bau, S. Liu, T. Wang, J.-Y. Zhu, A. Torralba. <a href="https://arxiv.org/abs/2007.15646">"Rewriting a Deep Generative Model"</a>. In ECCV 2020.</li><br>
    </ul>
    </td>
    </tr>
    </tbody></table>
    
    
          <br><hr>
    
            <table align="center" width="1100px">
              <tbody><tr>
                      <td width="400px">
                <left>
              <center><h2>Acknowledgements</h2></center>
              We thank Richard Zhang, Nupur Kumari, Gaurav Parmar, George Cazenavette for the helpful discussions. We thank Nupur Kumari and George Cazenavette again for the significant help with proof reading and writing suggestions. We are grateful to Ruihan Gao for the edit examples. We truly appreciate that Flower, Sheng-Yu's sister's cat, agreed to have her portrait edited in the figure.  S.-Y. Wang is partly supported by a Uber Presidential Fellowship. The work is partly supported by Adobe Inc. and Naver Corporation. Website template is from <a href="https://richzhang.github.io/colorization/">Colorful Colorization</a>.
          </left>
        </td>
           </tr>
        </tbody></table>
    
        <br><br>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7W9WKZS3SR"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-7W9WKZS3SR');
    </script>
    
    
    </body></html>
